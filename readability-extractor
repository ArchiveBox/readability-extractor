#!/usr/bin/env node
/*
readability-extractor
MIT License
https://github.com/ArchiveBox/readability-extractor

Usage:
  ./readability-extractor ./archive/12345678910/singlefile.html 'https://example.com/original/url.html' 'utf-8' > article.json
*/

const source_path = process.argv[2]                                                            // e.g. ./archive/12345678910/singlefile.html
const original_url = process.argv[3] || "https://example.com/url-not-specified"                // e.g. https://exmaple.com/some/page.html
const encoding_hint = process.argv[4] || null                                                  // e.g. utf-8 or windows-1252

// don't importing anything before this point to avoid erroring for missing dependencies during version query
if (process.argv.includes("--version")) {
    const package_info = require('./package.json')
    console.log(package_info.version)   // e.g. 1.0.0
    process.exit(0)
}

function getArticle(html) {
  const createDOMPurify = require('dompurify')
  const { Readability } = require('@mozilla/readability')
  const { JSDOM } = require('jsdom')

  const window = new JSDOM('').window

  // strip potential XSS/JS in the article using DOMPurify library
  const DOMPurify = createDOMPurify(window)
  const cleaned_html = DOMPurify.sanitize(html)
  const cleaned_dom = new JSDOM(cleaned_html, {url: original_url})
  
  // parse out the Article body text using Readability library
  const reader = new Readability(cleaned_dom.window.document)
  const article = reader.parse()

  // check for parse failure
  if (!article) {
    throw 'Readability was unable to parse article text from this HTML'
  }

  // Strip leading and trailing whitespace
  if (article['textContent'] !== null) {
      article['textContent'] = article['textContent'].trim()
  }

  // Attach information about original source charset to output
  const charset = cleaned_dom.window.document.characterSet
  article['charset'] = charset
  
  if (article['content'] !== null && charset !== 'windows-1252') {
      // prepend meta charset tag to html to hint to downstream renderers to use the correct original encoding
      article['content'] = `<meta charset="${charset}">\n${article['content']}`
  }
  return article
}

const fs = require('fs')

fs.readFile(source_path, 'utf8', function(err, data) {
    if (err) throw err;

    let html = data

    if (html.length > 14_000_000) {
      console.warn(`[!] WARNING: Stripping url(data:image/...,base64,...) inline images as HTML sorce is too long (${html.length}) to process within 512mb RAM limit!`)

      // Strip long url(data:image/...base64) strings created by singlefile as they cause JSDom to run out of memory if they are too big
      const base64UrlRegex = /data:.+?\/.+?;base64,.+?(\)|")/gm;
      const base64UrlReplacement = `data:image/png;base64,inline-images-stripped-by-archivebox-because-original-html-exceeded-size-limit==$1`;

      html = html.replaceAll(base64UrlRegex, base64UrlReplacement)
    }

    article = getArticle(html)

    console.log(JSON.stringify(article, null, 4))
});
